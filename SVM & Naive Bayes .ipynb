{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c2fb395-1b9b-4466-a96d-38a211e2c5f7",
   "metadata": {},
   "source": [
    "#### Assignment Code: DA-AG-013\n",
    "### SVM & Naive Bayes | Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabe8a5e-9d2c-4541-bfe1-c50ea67cc2c6",
   "metadata": {},
   "source": [
    "Question 1: What is a Support Vector Machine (SVM), and how does it work?\n",
    "Answer:\n",
    "-\n",
    "A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks, but it's mostly used for classification.\n",
    "\n",
    "The main idea of SVM is to find the best boundary (hyperplane) that separates data points of different classes.\n",
    "- In a 2D space, this boundary is a line.\n",
    "- In a 3D space, it is a plane.\n",
    "- In higher dimensions, it is a hyperplane.\n",
    "\n",
    "-> Key Concepts:\n",
    "\n",
    " 1.Hyperplane:\n",
    " - A decision boundary that separates different classes.\n",
    " - The goal is to choose the one that maximizes the margin between the classes.\n",
    "\n",
    " 2.Margin:\n",
    " - The distance between the hyperplane and the nearest data points from each class.\n",
    " - SVM tries to maximize this margin to ensure good separation.\n",
    "\n",
    " 3.Support Vectors:\n",
    " - The data points that are closest to the hyperplane.\n",
    " - These points are critical because they \"support\" the hyperplane ‚Äî if you remove them, the position of the hyperplane could change.\n",
    "\n",
    " 4.Kernel Trick:\n",
    " - If the data is not linearly separable, SVM uses a kernel function to transform the data into a higher dimension where it becomes linearly separable.\n",
    " - Common kernels:\n",
    "   - Linear\n",
    "   - Polynomial\n",
    "   - RBF (Radial Basis Function or Gaussian)\n",
    "\n",
    "-> Advantages of SVM:\n",
    " - Effective in high-dimensional spaces\n",
    " - Works well even when number of features > number of samples\n",
    " - Memory efficient (uses only support vectors)\n",
    "\n",
    "-> Disadvantages:\n",
    " - Not ideal for large datasets (training time is high)\n",
    " - Less effective when classes are heavily overlapping\n",
    " - Choosing the right kernel and tuning parameters can be tricky"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045c96e8-f24b-41ce-95c0-9a6cd2df4b77",
   "metadata": {},
   "source": [
    "Question 2: Explain the difference between Hard Margin and Soft Margin SVM.\n",
    "Answer:\n",
    "-\n",
    " 1.Hard Margin SVM:\n",
    "\n",
    "-> Definition:\n",
    "- A Hard Margin SVM assumes that the data is linearly separable ‚Äî i.e., you can draw a straight line (or hyperplane) that perfectly separates the two classes without any errors.\n",
    "\n",
    "-> Characteristics:\n",
    " - No misclassifications allowed.\n",
    " - Maximizes the margin strictly between the two classes.\n",
    " - Only works well when data is clean and clearly separable.\n",
    "\n",
    "-> Limitation:\n",
    " - Very sensitive to outliers and noisy data.\n",
    " - May overfit if the real-world data is not perfectly separable (which is often the case).\n",
    "\n",
    "\n",
    " 2.Soft Margin SVM:\n",
    "\n",
    "-> Definition:\n",
    " - A Soft Margin SVM allows some misclassification of data points to create a more flexible and generalizable model ‚Äî ideal for real-world, noisy, or overlapping data.\n",
    "\n",
    "-> Characteristics:\n",
    " - Introduces a penalty for misclassified points using a regularization parameter (C).\n",
    " - Balances margin maximization and classification error.\n",
    " - More robust and adaptable to complex datasets.\n",
    "\n",
    "-> Role of Parameter C:\n",
    " - Large C ‚Üí Less tolerance for errors (closer to hard margin).\n",
    " - Small C ‚Üí More tolerance for errors (larger margin, better generalization).\n",
    "\n",
    "\n",
    "#### Key Differences:\n",
    "\n",
    "| Feature                     | Hard Margin SVM                         | Soft Margin SVM             |\n",
    "| --------------------------- | --------------------------------------- | --------------------------- |\n",
    "| Assumes perfect separation? | Yes                                     | No                          |\n",
    "| Allows misclassifications?  | No                                      | Yes                         |\n",
    "| Works well on noisy data?   | Poorly                                  | Better                      |\n",
    "| Flexibility                 | Rigid (strict margin)                   | Flexible (controlled by C)  |\n",
    "| Risk of Overfitting         | High if data is not perfectly separable | Lower due to regularization |\n",
    "\n",
    "\n",
    "-> Example:\n",
    " - Imagine trying to draw a line between two groups of fruits (say apples and oranges):\n",
    "   - Hard Margin: Requires all apples on one side and all oranges on the other ‚Äî even if that means ignoring a few weird-shaped apples.\n",
    "   - Soft Margin: Allows a few fruits to be on the wrong side if that helps draw a better, more stable line overall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84a0730-83a2-4eb2-a7b7-6e6f2fd19584",
   "metadata": {},
   "source": [
    "Question 3: What is the Kernel Trick in SVM? Give one example of a kernel and explain its use case.\n",
    "Answer:\n",
    "-\n",
    "The Kernel Trick is a mathematical technique used in Support Vector Machines (SVMs) to transform non-linearly separable data into a higher-dimensional space where it can be separated by a linear hyperplane.\n",
    "\n",
    "Instead of actually computing the coordinates in the higher dimension (which is computationally expensive), the kernel trick computes the inner product between the images of all pairs of data in the feature space, using a kernel function.\n",
    "\n",
    "-> Why is it useful?\n",
    " - Real-world data is often not linearly separable.\n",
    " - The kernel trick allows SVM to find complex boundaries between classes without explicitly transforming the data.\n",
    " - Enables SVM to work with non-linear decision boundaries.\n",
    "\n",
    "-> How it works (Simple View):\n",
    "\n",
    "Imagine trying to separate a set of points shaped like concentric circles. In 2D, it's impossible to draw a straight line between the classes. But if we map the data into 3D (like lifting the inner circle upwards), a linear plane can separate them ‚Äî this is what the kernel trick helps SVM do, mathematically.\n",
    "\n",
    "- Example of a Kernel: RBF (Radial Basis Function) / Gaussian Kernel\n",
    "Formula:\n",
    "ùêæ(ùë•,x‚Ä≤)=exp‚Å°(‚àíùõæ‚à•ùë•‚àíùë•‚Ä≤‚à•2)\n",
    "\n",
    " - x, x': two feature vectors\n",
    " - Œ≥ (gamma): defines how far the influence of a single training example reaches.\n",
    "\n",
    "\n",
    "-> Use Case:\n",
    " - Used when the relationship between features and labels is highly non-linear.\n",
    " - Excellent for image classification, text categorization, or medical data, where patterns are complex.\n",
    "\n",
    "-> Real-life Example:\n",
    " - Handwritten digit recognition (like MNIST dataset): The digits are not linearly separable, but using the RBF kernel, SVM can learn complex boundaries between digit classes like '3' and '8'.\n",
    "\n",
    "#### Other Common Kernels (for reference):\n",
    "| Kernel Type    | Use Case                                                                                               |\n",
    "| -------------- | ------------------------------------------------------------------------------------------------------ |\n",
    "| **Linear**     | When data is linearly separable or has many features (e.g., text data)                                 |\n",
    "| **Polynomial** | When interaction between features matters (e.g., image classification with curved decision boundaries) |\n",
    "| **Sigmoid**    | Used in neural networks, behaves like a two-layer perceptron                                           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f5eb5e-ec56-490e-9308-e1762db83669",
   "metadata": {},
   "source": [
    "Question 4: What is a Na√Øve Bayes Classifier, and why is it called ‚Äúna√Øve‚Äù?\n",
    "Answer:\n",
    "-\n",
    "The Na√Øve Bayes Classifier is a supervised machine learning algorithm based on Bayes‚Äô Theorem, used primarily for classification tasks.\n",
    "\n",
    "It‚Äôs called \"na√Øve\" because it assumes that all features are independent of each other, which is rarely true in real-world data ‚Äî hence, the name.\n",
    "\n",
    "-> Bayes‚Äô Theorem (Simplified):\n",
    "\n",
    "P(A‚à£B)=P(B‚à£A)‚ãÖP(A) / P(B)\n",
    "\n",
    "-> In classification:\n",
    " - P(A‚à£B): Probability of class A given features B\n",
    " - P(B‚à£A): Probability of features B given class A\n",
    " - P(A): Prior probability of class A\n",
    " - P(B): Probability of features B\n",
    "\n",
    " How Na√Øve Bayes Works (Step-by-step):\n",
    " - Calculate Prior Probability for each class (e.g., spam vs. not spam).\n",
    " - Calculate Likelihood: For each feature, compute the probability of that feature occurring in a given class.\n",
    " - Apply Bayes‚Äô Theorem to get the posterior probability for each class.\n",
    " - Choose the class with the highest posterior probability as the prediction.\n",
    "\n",
    "Why is it called ‚ÄúNa√Øve‚Äù?\n",
    " - Because it na√Øvely assumes that all features are independent.\n",
    " - For example, if you're classifying emails, it assumes that the occurrence of the word \"free\" is independent of the word \"offer\", even though they often appear together.\n",
    " - Despite this simplifying assumption, Na√Øve Bayes often performs surprisingly well, especially in text classification.\n",
    "\n",
    "\n",
    "Example Use Cases:\n",
    " - Spam filtering\n",
    " - Sentiment analysis\n",
    " - Document classification\n",
    " - Medical diagnosis (with categorical features)\n",
    "\n",
    "\n",
    "#### Types of Na√Øve Bayes:\n",
    "| Type                        | Used When                                                 |\n",
    "| --------------------------- | --------------------------------------------------------- |\n",
    "| **Gaussian Na√Øve Bayes**    | Features are continuous and follow a normal distribution  |\n",
    "| **Multinomial Na√Øve Bayes** | Features are counts (e.g., word frequencies in text data) |\n",
    "| **Bernoulli Na√Øve Bayes**   | Features are binary (e.g., word present or not)           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b339c8b-88f9-4cf3-bc3a-c89a0d120f12",
   "metadata": {},
   "source": [
    "Question 5: Describe the Gaussian, Multinomial, and Bernoulli Na√Øve Bayes variants. When would you use each one?\n",
    "Answer:\n",
    "-\n",
    "Na√Øve Bayes has three main variants, each designed for different types of input features.\n",
    "\n",
    " 1.Gaussian Na√Øve Bayes\n",
    "\n",
    "Description:\n",
    " - Assumes that continuous features follow a normal (Gaussian) distribution.\n",
    " - For each feature, it calculates the mean and standard deviation for each class.\n",
    " - Then, it uses the Gaussian probability density function to compute the likelihood of features.\n",
    "\n",
    "- Formula:\n",
    "\n",
    "ùëÉ(ùë•ùëñ‚à£ùë¶)=12ùúãùúé2exp(‚àí(ùë•ùëñ‚àíùúá)22ùúé2)P(xi‚Äã ‚à£y)= 2œÄœÉ 2 ‚Äã 1‚Äã exp(‚àí 2œÉ 2\n",
    " \n",
    "Use Case:\n",
    " - When features are real-valued and continuous, e.g.:\n",
    "   - Medical data (e.g., height, weight, blood pressure)\n",
    "   - Sensor readings\n",
    "   - Iris flower dataset\n",
    "\n",
    " 2.Multinomial Na√Øve Bayes\n",
    "\n",
    "Description:\n",
    " - Assumes that features represent discrete frequency counts (e.g., how often a word appears).\n",
    " - Commonly used for document classification or text analysis.\n",
    "\n",
    "Use Case:\n",
    " - When features are counts or frequencies, e.g.:\n",
    "   - Text classification (e.g., spam detection, sentiment analysis)\n",
    "   - Bag-of-Words or TF-IDF vectors\n",
    "\n",
    "Example:\n",
    " - Email classification based on the number of times each word appears.\n",
    "\n",
    "\n",
    " 3.Bernoulli Na√Øve Bayes\n",
    "\n",
    "Description:\n",
    " - Designed for binary/boolean features (0 or 1).\n",
    " - Assumes features are either present or absent (not counts).\n",
    " - Models feature presence/absence independently for each class.\n",
    "\n",
    "Use Case:\n",
    " - When features are binary, e.g.:\n",
    "   - Whether a specific word appears or not in a document\n",
    "   - Text classification with binary features\n",
    "   - Market basket analysis (item purchased: yes/no)\n",
    "\n",
    "Example:\n",
    " - Spam filtering based on whether words like ‚Äúfree‚Äù or ‚Äúwin‚Äù are present.\n",
    "\n",
    "#### Comparison Table:\n",
    "\n",
    "| Variant         | Feature Type    | Best For                                  |\n",
    "| --------------- | --------------- | ----------------------------------------- |\n",
    "| **Gaussian**    | Continuous      | Sensor data, numerical features           |\n",
    "| **Multinomial** | Discrete counts | Text classification with word frequencies |\n",
    "| **Bernoulli**   | Binary (0 or 1) | Binary text features or presence/absence  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1716554-6a4b-4437-9e9b-431471a1c87d",
   "metadata": {},
   "source": [
    "Dataset Info:\n",
    "‚óè You can use any suitable datasets like Iris, Breast Cancer, or Wine from\n",
    "sklearn.datasets or a CSV file you have.\n",
    "\n",
    "Question 6: Write a Python program to:\n",
    "‚óè Load the Iris dataset\n",
    "‚óè Train an SVM Classifier with a linear kernel\n",
    "‚óè Print the model's accuracy and support vectors.\n",
    "(Include your Python code and output in the code box below.)\n",
    "Answer:\n",
    "-\n",
    "Python Program using SVM on Iris Dataset\n",
    "\n",
    "Here is a Python program that:\n",
    " - Loads the Iris dataset\n",
    " - Trains a Support Vector Machine (SVM) classifier with a linear kernel\n",
    " - Prints the accuracy and the support vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a28b3f08-8d9d-4d24-9d37-eba822c6c342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 1.0\n",
      "Number of Support Vectors for each class: [ 3 11 11]\n",
      "Support Vectors:\n",
      " [[4.8 3.4 1.9 0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.7 3.  5.  1.7]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [5.9 3.  5.1 1.8]\n",
      " [4.9 2.5 4.5 1.7]]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split into training and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train an SVM Classifier with a linear kernel\n",
    "svm_model = SVC(kernel='linear')\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print accuracy and support vectors\n",
    "print(\"Model Accuracy:\", accuracy)\n",
    "print(\"Number of Support Vectors for each class:\", svm_model.n_support_)\n",
    "print(\"Support Vectors:\\n\", svm_model.support_vectors_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f79509-fe5a-4ec2-8eb9-568fd8a93387",
   "metadata": {},
   "source": [
    "Question 7: Write a Python program to:\n",
    "‚óè Load the Breast Cancer dataset\n",
    "‚óè Train a Gaussian Na√Øve Bayes model\n",
    "‚óè Print its classification report including precision, recall, and F1-score.\n",
    "(Include your Python code and output in the code box below.)\n",
    "Answer:\n",
    "-\n",
    "Python Program using Gaussian Na√Øve Bayes on Breast Cancer Dataset\n",
    "\n",
    "This program:\n",
    " - Loads the Breast Cancer dataset from sklearn.datasets\n",
    " - Trains a Gaussian Na√Øve Bayes classifier\n",
    " - Prints the classification report including precision, recall, and F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0f8fd27-12ee-44fe-bd43-d93c0a16b18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   malignant       1.00      0.93      0.96        43\n",
      "      benign       0.96      1.00      0.98        71\n",
      "\n",
      "    accuracy                           0.97       114\n",
      "   macro avg       0.98      0.97      0.97       114\n",
      "weighted avg       0.97      0.97      0.97       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load the Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split into training and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Gaussian Naive Bayes model\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred, target_names=data.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743eb524-6d47-43ed-80c5-4342a2ee45f7",
   "metadata": {},
   "source": [
    "Question 8: Write a Python program to:\n",
    "‚óè Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best\n",
    "C and gamma.\n",
    "‚óè Print the best hyperparameters and accuracy.\n",
    "(Include your Python code and output in the code box below.)\n",
    "Answer:\n",
    "-\n",
    "This program:\n",
    " - Loads the Wine dataset\n",
    " - Trains an SVM classifier\n",
    " - Uses GridSearchCV to find the best combination of C and gamma\n",
    " - Prints the best hyperparameters and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75fcf078-a1cc-49cb-afa3-35eb96c40fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "Test Set Accuracy: 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Wine dataset\n",
    "data = load_wine()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the data into training and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the SVM model\n",
    "svm = SVC()\n",
    "\n",
    "# Set up the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [1, 0.1, 0.01, 0.001],\n",
    "    'kernel': ['rbf']\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid = GridSearchCV(svm, param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = grid.predict(X_test)\n",
    "\n",
    "# Print best hyperparameters and accuracy\n",
    "print(\"Best Hyperparameters:\", grid.best_params_)\n",
    "print(\"Test Set Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc0deb1-1f15-4e32-95bf-e83300a591a8",
   "metadata": {},
   "source": [
    "Question 9: Write a Python program to:\n",
    "‚óè Train a Na√Øve Bayes Classifier on a synthetic text dataset (e.g. using\n",
    "sklearn.datasets.fetch_20newsgroups).\n",
    "‚óè Print the model's ROC-AUC score for its predictions.\n",
    "(Include your Python code and output in the code box below.)\n",
    "Answer:\n",
    "-\n",
    "This program:\n",
    " - Loads a synthetic text dataset (fetch_20newsgroups)\n",
    " - Trains a Na√Øve Bayes classifier on text data\n",
    " - Calculates and prints the ROC-AUC score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b7ba662b-c454-49d7-a774-12fc912053f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC Score: 0.9929591836734695\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Load a subset of the 20 newsgroups dataset (binary classification for ROC-AUC)\n",
    "categories = ['sci.med', 'rec.autos']  # 2 classes for binary classification\n",
    "data = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Vectorize the text data\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(data.data)\n",
    "y = data.target\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Naive Bayes classifier\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities for the positive class\n",
    "y_proba = nb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute ROC-AUC score\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "# Print ROC-AUC score\n",
    "print(\"ROC-AUC Score:\", roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b375126f-f07b-49bb-a5a6-52e571d3aab7",
   "metadata": {},
   "source": [
    "ROC-AUC Score: 0.9841|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e339817a-669e-4541-8433-53eb0a1cd259",
   "metadata": {},
   "source": [
    "Question 10: Imagine you‚Äôre working as a data scientist for a company that handles\n",
    "email communications.\n",
    "Your task is to automatically classify emails as Spam or Not Spam. The emails may\n",
    "contain:\n",
    "‚óè Text with diverse vocabulary\n",
    "‚óè Potential class imbalance (far more legitimate emails than spam)\n",
    "‚óè Some incomplete or missing data\n",
    "Explain the approach you would take to:\n",
    "‚óè Preprocess the data (e.g. text vectorization, handling missing data)\n",
    "‚óè Choose and justify an appropriate model (SVM vs. Na√Øve Bayes)\n",
    "‚óè Address class imbalance\n",
    "‚óè Evaluate the performance of your solution with suitable metrics\n",
    "And explain the business impact of your solution.\n",
    "(Include your Python code and output in the code box below.)\n",
    "Answer:\n",
    "-\n",
    "Spam Detection Using Machine Learning\n",
    "\n",
    "Imagine you're tasked with building an email spam classifier using real-world email data that has:\n",
    " - Text content\n",
    " - Missing values\n",
    " - Class imbalance (more \"Not Spam\" than \"Spam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0aab99b-5a4a-4ee6-bee2-8099b8506615",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.Preprocessing the Data\n",
    "\n",
    "##Handling Missing Data:\n",
    "\n",
    "# Replace missing values with empty strings\n",
    "import pandas as pd\n",
    "data['email_text'] = data['email_text'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8ada6a-2055-4bfe-8789-f0bdc9a5d7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Vectorization (Convert text to numeric):\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Convert text to TF-IDF vectors\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_df=0.95)\n",
    "X = vectorizer.fit_transform(data['email_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd0bdfb-0f15-4d1e-ae5c-0b97a893ca40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Target Variable:\n",
    "\n",
    "y = data['label']  # 0 = Not Spam, 1 = Spam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2e7e82-500f-4309-8645-83dd7879057e",
   "metadata": {},
   "source": [
    "#### 2.Choosing the Model: SVM vs Na√Øve Bayes\n",
    "\n",
    "| Model           | Pros                                            | Cons                                     |\n",
    "| --------------- | ----------------------------------------------- | ---------------------------------------- |\n",
    "| **Na√Øve Bayes** | Fast, handles high-dimensional sparse data well | Assumes feature independence             |\n",
    "| **SVM**         | Accurate and robust with kernels                | Slower with large datasets, needs tuning |\n",
    "\n",
    " Choice: Na√Øve Bayes (Multinomial) ‚Äî Ideal for text classification, faster, handles word frequency vectors effectively.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9088a1eb-47a2-4460-ac1d-8921e19111d4",
   "metadata": {},
   "source": [
    "#### 3.Handling Class Imbalance\n",
    " - Spam vs Not Spam = Imbalanced\n",
    " - Use class_weight (for SVM) or resampling techniques\n",
    "\n",
    "Example: Use SMOTE to oversample the minority class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ada3f4-4d9f-40a0-8b7d-ab21b32f1880",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE()\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4414aa-05bc-451c-bc80-8e523333f5ea",
   "metadata": {},
   "source": [
    "#### 4.Evaluation Metrics\n",
    "Because of imbalance, accuracy is not enough. Use:\n",
    " - Precision (how many predicted spams are actually spam)\n",
    " - Recall (how many actual spams were correctly identified)\n",
    " - F1-Score (balance between precision & recall)\n",
    " - ROC-AUC Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "802d64f7-be2c-49e4-a1bf-701118a7ada9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.99      0.96       193\n",
      "           1       0.99      0.91      0.95       161\n",
      "\n",
      "    accuracy                           0.96       354\n",
      "   macro avg       0.96      0.95      0.96       354\n",
      "weighted avg       0.96      0.96      0.96       354\n",
      "\n",
      "ROC-AUC Score: 0.9909889614778105\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "# Simulate spam dataset using 2 categories\n",
    "categories = ['talk.politics.misc', 'rec.sport.baseball']  # Assume one is spam-like\n",
    "data = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Create a DataFrame\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'email_text': data.data, 'label': data.target})\n",
    "\n",
    "# Fill missing text\n",
    "df['email_text'] = df['email_text'].fillna('')\n",
    "\n",
    "# TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_df=0.95)\n",
    "X = vectorizer.fit_transform(df['email_text'])\n",
    "y = df['label']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model: Multinomial Na√Øve Bayes\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and Probabilities\n",
    "y_pred = model.predict(X_test)\n",
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluation\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22d6c73-0150-4fb3-b2f0-b0e8c658221e",
   "metadata": {},
   "outputs": [],
   "source": [
    "####"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
